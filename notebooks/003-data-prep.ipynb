{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Our goal is to get a small sample of the dataset to work with that is good enough to iterate on. A small sample makes it faster to iterate, hence enable easier debugging. If we can not make our model work on a reasonably small dataset then it's not likely to work on bigger one either.\n",
    "\n",
    "We define a sufficiently small training dataset as:\n",
    "- **Richness**: Every user should have at least 5 interactions and each item should have at least 10 interactions\n",
    "- **Enough samples**: There should be at least 1000 users in the training dataset and about 1000 interactions in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel, model_validator\n",
    "from load_dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from src.visualization.setup import FSDSColors\n",
    "_ = load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"run_name\": \"003-prep-data\",\n",
      "  \"testing\": false,\n",
      "  \"notebook_persist_dp\": \"/home/dinhln/Desktop/MLOPS/recsys/HM-ScalableRecs/data/interim\",\n",
      "  \"random_seed\": 41,\n",
      "  \"user_col\": \"customer_id\",\n",
      "  \"item_col\": \"article_id\",\n",
      "  \"interaction_col\": \"price\",\n",
      "  \"timestamp_col\": \"t_dat\",\n",
      "  \"sample_users\": 1000,\n",
      "  \"min_user_interactions\": 5,\n",
      "  \"min_item_interactions\": 10,\n",
      "  \"user\": \"lastfirstkiss\",\n",
      "  \"password\": \"nightchange\",\n",
      "  \"db\": \"hm-recsys\",\n",
      "  \"host\": \"localhost\",\n",
      "  \"port\": 5432\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class Args(BaseModel):\n",
    "    run_name: str = \"003-prep-data\"\n",
    "    testing: bool = False\n",
    "    notebook_persist_dp: str = None\n",
    "    random_seed: int = 41\n",
    "\n",
    "    user_col: str = \"customer_id\"\n",
    "    item_col: str = \"article_id\"\n",
    "    interaction_col: str = \"price\"\n",
    "    timestamp_col: str = \"t_dat\"\n",
    "\n",
    "    sample_users: int = 1000\n",
    "    min_user_interactions: int = 5\n",
    "    min_item_interactions: int = 10\n",
    "\n",
    "    # Database credentials\n",
    "    user: str = None\n",
    "    password: str = None\n",
    "    db: str = None\n",
    "    host: str = None\n",
    "    port: int = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def load_env_vars(cls, values):\n",
    "        # Load environment variables if not explicitly set\n",
    "        values[\"user\"] = values.get(\"user\") or os.getenv(\"POSTGRES_USER\")\n",
    "        values[\"password\"] = values.get(\"password\") or os.getenv(\"POSTGRES_PASSWORD\")\n",
    "        values[\"db\"] = values.get(\"db\") or os.getenv(\"POSTGRES_DB\")\n",
    "        values[\"host\"] = values.get(\"host\") or os.getenv(\"POSTGRES_HOST\")\n",
    "        values[\"port\"] = values.get(\"port\") or os.getenv(\"POSTGRES_PORT\")\n",
    "        return values\n",
    "\n",
    "    def init(self):\n",
    "        self.notebook_persist_dp = os.path.abspath(f\"../data/interim\")\n",
    "        if not self.testing:\n",
    "            os.makedirs(self.notebook_persist_dp, exist_ok=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "args = Args().init()\n",
    "\n",
    "print(args.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from OLAP (assume PostgreSQL in this context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       t_dat                                        customer_id  article_id  \\\n",
      "0 2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   663713001   \n",
      "1 2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   541518023   \n",
      "2 2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...   505221004   \n",
      "3 2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...   685687003   \n",
      "4 2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...   685687004   \n",
      "\n",
      "      price  sales_channel_id  \n",
      "0  0.050831                 2  \n",
      "1  0.030492                 2  \n",
      "2  0.015237                 2  \n",
      "3  0.016932                 2  \n",
      "4  0.016932                 2  \n"
     ]
    }
   ],
   "source": [
    "# Define the PostgreSQL connection URL\n",
    "DATABASE_URL = f\"postgresql+psycopg2://{args.user}:{args.password}@{args.host}:{args.port}/{args.db}\"\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Write a SQL query\n",
    "query = \"SELECT * FROM transactions;\"\n",
    "\n",
    "# Load data into a Pandas DataFrame\n",
    "with engine.connect() as connection:\n",
    "    data = pd.read_sql(query, con=connection)\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t_dat               datetime64[ns]\n",
       "customer_id                 object\n",
       "article_id                   int64\n",
       "price                      float64\n",
       "sales_channel_id             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling data\n",
    "\n",
    "Just randomly get X users will not guarantee that the output dataset would qualify the condition of **richness**. Instead we take an iterative approach where we gradually drop random users from the dataset while keeping an eye on the conditions and our sampling target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_users(df, k=10):\n",
    "    users = df[args.user_col].unique()\n",
    "    np.random.seed(args.random_seed)\n",
    "    to_remove_users = np.random.choice(users, size=k, replace=False)\n",
    "    return df.loc[lambda df: ~df[args.user_col].isin(to_remove_users)]\n",
    "\n",
    "\n",
    "def get_unqualified(df, col: str, threshold: int):\n",
    "    unqualified = df.groupby(col).size().loc[lambda s: s < threshold].index\n",
    "    return unqualified\n",
    "\n",
    "\n",
    "get_unqualified_users = partial(\n",
    "    get_unqualified, col=args.user_col, threshold=args.min_user_interactions\n",
    ")\n",
    "get_unqualified_items = partial(\n",
    "    get_unqualified, col=args.item_col, threshold=args.min_item_interactions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_perc = 0.2\n",
    "perc_users_removed_each_round = 0.01\n",
    "debug = True\n",
    "min_val_records = 1000\n",
    "\n",
    "keep_random_removing = True\n",
    "r = 1\n",
    "\n",
    "sample_df = data.copy()\n",
    "\n",
    "while keep_random_removing:\n",
    "    num_users_removed_each_round = int(\n",
    "        perc_users_removed_each_round * sample_df[args.user_col].nunique()\n",
    "    )\n",
    "    print(\n",
    "        f\"\\n\\nRandomly removing {num_users_removed_each_round} users - Round {r} started\"\n",
    "    )\n",
    "    new_sample_df = remove_random_users(sample_df, k=num_users_removed_each_round)\n",
    "\n",
    "    keep_removing = True\n",
    "    i = 1\n",
    "\n",
    "    while keep_removing:\n",
    "        if debug:\n",
    "            logger.info(f\"Sampling round {i} started\")\n",
    "        keep_removing = False\n",
    "        uu = get_unqualified_users(new_sample_df)\n",
    "        if debug:\n",
    "            logger.info(f\"{len(uu)=}\")\n",
    "        if len(uu):\n",
    "            new_sample_df = new_sample_df.loc[lambda df: ~df[args.user_col].isin(uu)]\n",
    "            if debug:\n",
    "                logger.info(f\"After removing uu: {len(new_sample_df)=}\")\n",
    "            assert len(get_unqualified_users(new_sample_df)) == 0\n",
    "            keep_removing = True\n",
    "        ui = get_unqualified_items(new_sample_df)\n",
    "        if debug:\n",
    "            logger.info(f\"{len(ui)=}\")\n",
    "        if len(ui):\n",
    "            new_sample_df = new_sample_df.loc[lambda df: ~df[args.item_col].isin(ui)]\n",
    "            if debug:\n",
    "                logger.info(f\"After removing ui: {len(new_sample_df)=}\")\n",
    "            assert len(get_unqualified_items(new_sample_df)) == 0\n",
    "            keep_removing = True\n",
    "        i += 1\n",
    "\n",
    "    sample_users = sample_df[args.user_col].unique()\n",
    "    sample_items = sample_df[args.item_col].unique()\n",
    "    num_users = len(sample_users)\n",
    "    logger.info(f\"After randomly removing users - round {r}: {num_users=}\")\n",
    "    if num_users > args.sample_users * (1 + buffer_perc):\n",
    "        logger.info(\n",
    "            f\"Number of users {num_users} are still greater than expected, keep removing...\"\n",
    "        )\n",
    "        sample_df = new_sample_df.copy()\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Number of users {num_users} are falling below expected threshold, stop and use `sample_df` as final output...\"\n",
    "        )\n",
    "        keep_random_removing = False\n",
    "\n",
    "    val_sample_df = val_raw.loc[\n",
    "        lambda df: df[args.user_col].isin(sample_users)\n",
    "        & df[args.item_col].isin(sample_items)\n",
    "    ]\n",
    "    if (num_val_records := val_sample_df.shape[0]) < min_val_records:\n",
    "        logger.info(\n",
    "            f\"Number of val_df records {num_val_records} are falling below expected threshold, stop and use `sample_df` as final output...\"\n",
    "        )\n",
    "        keep_random_removing = False\n",
    "\n",
    "    r += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
